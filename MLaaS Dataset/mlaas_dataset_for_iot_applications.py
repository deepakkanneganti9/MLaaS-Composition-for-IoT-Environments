# -*- coding: utf-8 -*-
"""MLaaS Dataset for IoT Applications.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aA7IthWWmvQBbvryzt69T2E23CFFX4_I

**MNIST Dataset**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS        = 20      # you can reduce for faster runs
IID_FRACTION       = 0.4     # half IID, half non-IID
LOCAL_EPOCHS       = 20
GLOBAL_ROUNDS      = 10     # rounds for quality & reliability
BATCH_SIZE         = 64
AVAILABILITY_PROB  = 0.6     # probability a client participates in a round

# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/MLaaS_Weights_20_MNIST"  # <--- UPDATED LOCATION
os.makedirs(WEIGHTS_DIR, exist_ok=True)

FEATURE_COUNT = 28 * 28

# ======================================================
# LOAD MNIST
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0
y_train = y_train.astype("int32")
y_test  = y_test.astype("int32")

all_images = np.concatenate([x_train, x_test], axis=0)
all_labels = np.concatenate([y_train, y_test], axis=0)


def create_clients_iid_noniid(images, labels, num_clients, iid_indices, non_iid_indices):
    clients = {}
    scenario = {}

    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        samples_per_client = np.random.randint(800, 4000)
        client_samples = []

        if i in iid_indices:
            per_class = samples_per_client // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls], per_class, replace=True)
                client_samples.extend(chosen)
            scen = "IID"

        elif i in non_iid_indices:
            alpha = np.random.uniform(0.3, 3.0)
            class_weights = np.random.dirichlet(np.ones(10) * alpha)

            for cls in range(10):
                n_cls = int(class_weights[cls] * samples_per_client)
                if n_cls > 0:
                    chosen = np.random.choice(class_indices[cls], n_cls, replace=True)
                    client_samples.extend(chosen)
            scen = "NonIID"

        np.random.shuffle(client_samples)
        cid = i + 1
        clients[cid] = (images[client_samples], labels[client_samples])
        scenario[cid] = scen

    return clients, scenario


num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_indices     = list(range(0, num_iid))
non_iid_indices = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients_iid_noniid(
    all_images, all_labels,
    NUM_CLIENTS, iid_indices, non_iid_indices
)

print(f"âœ… Created {len(clients)} clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")


# ======================================================
# MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(16, (3, 3), activation="relu", input_shape=(28, 28, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(32, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


# ======================================================
# LOCAL TRAINING + LOCAL QoS
# ======================================================
client_records = []
base_accuracy = {}
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    x_c = x_c[..., None]

    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c,
              epochs=LOCAL_EPOCHS,
              batch_size=BATCH_SIZE,
              verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000.0

    _, acc = model.evaluate(x_test[..., None], y_test, verbose=0)
    acc_pct = float(acc * 100.0)
    base_accuracy[cid] = acc_pct  # kept (still useful for reference, not used in inactive rounds)

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    bw_time = time.time() - bw_t0
    BW_MBps = float(size_MB / bw_time)

    # ======================================================
    # SAVE CLIENT WEIGHTS IN GOOGLE DRIVE
    # ======================================================
    weight_path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(weight_path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    label_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": int(len(y_c)),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": weight_path,
    }

    for l in range(10):
        row[f"Label{l}"] = int(label_map.get(l, 0))

    client_records.append(row)

    local_results_for_global[cid] = {
        "samples": len(y_c),
        "data": (x_c, y_c),
    }

print(f"âœ… Local training finished for {len(client_records)} clients")


# ======================================================
# FEDERATED ROUNDS
# ======================================================
def fedavg_weights(active_clients, local_results, global_model_weights):
    total_samples = sum(local_results[c]["samples"] for c in active_clients)
    if total_samples == 0:
        return global_model_weights

    agg = [np.zeros_like(w) for w in global_model_weights]

    for c in active_clients:
        x_c, y_c = local_results[c]["data"]
        model = build_model()
        model.set_weights(global_model_weights)
        model.fit(x_c, y_c, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w_c = model.get_weights()
        weight_factor = local_results[c]["samples"] / total_samples
        for i, w in enumerate(w_c):
            agg[i] += w * weight_factor

    return agg


quality_history = {cid: [] for cid in clients.keys()}
availability_history = {cid: [] for cid in clients.keys()}

global_model = build_model()
global_weights = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active_clients = []

    for cid in clients.keys():
        is_active = (np.random.rand() < AVAILABILITY_PROB)
        availability_history[cid].append(1 if is_active else 0)
        if is_active:
            active_clients.append(cid)

    if active_clients:
        global_weights = fedavg_weights(
            active_clients, local_results_for_global, global_weights
        )
        global_model.set_weights(global_weights)

    # ======================================================
    # UPDATED: Quality_Factor uses 0 when client is inactive
    # ======================================================
    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1] == 1:
            _, acc_c = global_model.evaluate(x_c[..., None], y_c, verbose=0)
            acc_value = float(acc_c * 100.0)
        else:
            acc_value = 0.0  # âœ… inactive => no task performed => quality = 0

        quality_history[cid].append(acc_value)

    print(f"Round {rnd}: active clients = {len(active_clients)}")


# ======================================================
# RELIABILITY SCORE
# ======================================================
reliability_score = {}
for cid in clients.keys():
    activations = sum(availability_history[cid])
    reliability_score[cid] = activations / GLOBAL_ROUNDS

print("âœ… Quality_Factor (per-round accuracies w/ zeros for inactive) and Reliability_Score computed.")


# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)
df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(
    lambda cid: json.dumps(quality_history[int(cid)])
)
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(
    lambda cid: float(np.mean(quality_history[int(cid)]))
)
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(
    lambda cid: reliability_score[int(cid)]
)
print(df_clients.head())

df_clients

df_clients.to_csv("MNIST_Client_Profiles_For_Composability_20_20.csv")

df_clients.columns

df_clients['C_p'].describe()

df_clients['Local_Accuracy(%)'].describe()

df_clients.columns

"""**FMNIST Dataset**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS        = 30      # you can reduce for faster runs
IID_FRACTION       = 0.4      # fraction of IID clients
LOCAL_EPOCHS       = 20
GLOBAL_ROUNDS      = 10        # rounds for quality & reliability
BATCH_SIZE         = 64
AVAILABILITY_PROB  = 0.6      # probability a client participates in a round

# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/MLaaS_Weights_30_FMNIST"  # <--- UPDATED LOCATION
os.makedirs(WEIGHTS_DIR, exist_ok=True)

FEATURE_COUNT = 28 * 28

# ======================================================
# LOAD FASHION-MNIST
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0
y_train = y_train.astype("int32")
y_test  = y_test.astype("int32")

all_images = np.concatenate([x_train, x_test], axis=0)
all_labels = np.concatenate([y_train, y_test], axis=0)

# ======================================================
# CREATE CLIENTS (IID + NonIID)  âœ… same as your logic
# ======================================================
def create_clients_iid_noniid(images, labels, num_clients, iid_indices, non_iid_indices):
    clients = {}
    scenario = {}

    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        samples_per_client = np.random.randint(800, 4000)
        client_samples = []

        if i in iid_indices:
            per_class = samples_per_client // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls], per_class, replace=True)
                client_samples.extend(chosen)
            scen = "IID"

        elif i in non_iid_indices:
            alpha = np.random.uniform(0.3, 3.0)
            class_weights = np.random.dirichlet(np.ones(10) * alpha)

            for cls in range(10):
                n_cls = int(class_weights[cls] * samples_per_client)
                if n_cls > 0:
                    chosen = np.random.choice(class_indices[cls], n_cls, replace=True)
                    client_samples.extend(chosen)
            scen = "NonIID"

        np.random.shuffle(client_samples)
        cid = i + 1
        clients[cid] = (images[client_samples], labels[client_samples])
        scenario[cid] = scen

    return clients, scenario

num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_indices     = list(range(0, num_iid))
non_iid_indices = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients_iid_noniid(
    all_images, all_labels,
    NUM_CLIENTS, iid_indices, non_iid_indices
)

print(f"âœ… Created {len(clients)} clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")

# ======================================================
# MODEL  âœ… same as your logic
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(16, (3, 3), activation="relu", input_shape=(28, 28, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(32, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model

# ======================================================
# LOCAL TRAINING + LOCAL QoS  âœ… same algorithm, same metrics
# ======================================================
client_records = []
base_accuracy = {}               # kept only for reference
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    x_c = x_c[..., None]

    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c,
              epochs=LOCAL_EPOCHS,
              batch_size=BATCH_SIZE,
              verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000.0

    _, acc = model.evaluate(x_test[..., None], y_test, verbose=0)
    acc_pct = float(acc * 100.0)
    base_accuracy[cid] = acc_pct  # kept (not used when inactive anymore)

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    bw_time = time.time() - bw_t0
    BW_MBps = float(size_MB / bw_time)

    # ======================================================
    # SAVE CLIENT WEIGHTS IN GOOGLE DRIVE âœ… same as MNIST final version
    # ======================================================
    weight_path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(weight_path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    label_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": int(len(y_c)),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": weight_path,
    }

    for l in range(10):
        row[f"Label{l}"] = int(label_map.get(l, 0))

    client_records.append(row)

    local_results_for_global[cid] = {
        "samples": len(y_c),
        "data": (x_c, y_c),
    }

print(f"âœ… Local training finished for {len(client_records)} clients")

# ======================================================
# FEDERATED ROUNDS  âœ… same FedAvg flow, but quality=0 if inactive
# ======================================================
def fedavg_weights(active_clients, local_results, global_model_weights):
    total_samples = sum(local_results[c]["samples"] for c in active_clients)
    if total_samples == 0:
        return global_model_weights

    agg = [np.zeros_like(w) for w in global_model_weights]

    for c in active_clients:
        x_c, y_c = local_results[c]["data"]
        model = build_model()
        model.set_weights(global_model_weights)
        model.fit(x_c, y_c, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w_c = model.get_weights()
        weight_factor = local_results[c]["samples"] / total_samples
        for i, w in enumerate(w_c):
            agg[i] += w * weight_factor

    return agg

quality_history = {cid: [] for cid in clients.keys()}
availability_history = {cid: [] for cid in clients.keys()}

global_model = build_model()
global_weights = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active_clients = []

    for cid in clients.keys():
        is_active = (np.random.rand() < AVAILABILITY_PROB)
        availability_history[cid].append(1 if is_active else 0)
        if is_active:
            active_clients.append(cid)

    if active_clients:
        global_weights = fedavg_weights(
            active_clients, local_results_for_global, global_weights
        )
        global_model.set_weights(global_weights)

    # ======================================================
    # UPDATED TO MATCH YOUR FINAL VERSION:
    # inactive => no task performed => quality = 0
    # ======================================================
    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1] == 1:
            _, acc_c = global_model.evaluate(x_c[..., None], y_c, verbose=0)
            acc_value = float(acc_c * 100.0)
        else:
            acc_value = 0.0  # âœ… inactive => 0 (same as your MNIST final version)

        quality_history[cid].append(acc_value)

    print(f"Round {rnd}: active clients = {len(active_clients)}")

# ======================================================
# RELIABILITY SCORE
# ======================================================
reliability_score = {}
for cid in clients.keys():
    activations = sum(availability_history[cid])
    reliability_score[cid] = activations / GLOBAL_ROUNDS

print("âœ… Quality_Factor (per-round accuracies w/ zeros for inactive) and Reliability_Score computed.")

# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)

df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(
    lambda cid: json.dumps(quality_history[int(cid)])
)

df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(
    lambda cid: float(np.mean(quality_history[int(cid)]))
)

df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(
    lambda cid: reliability_score[int(cid)]
)

print(df_clients.head())

df_clients.describe()

df_clients.to_csv("FMNIST_Client_Profiles_For_Composability_30_30.csv")

"""**CIFAR-10**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS        = 30      # you can reduce for faster runs
IID_FRACTION       = 0.4      # fraction of IID clients
LOCAL_EPOCHS       = 30
GLOBAL_ROUNDS      = 10        # rounds for quality & reliability
BATCH_SIZE         = 64
AVAILABILITY_PROB  = 0.6      # probability a client participates in a round

# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/MLaaS_Weights_20_CIFAR"  # <--- UPDATED LOCATION
os.makedirs(WEIGHTS_DIR, exist_ok=True)

FEATURE_COUNT = 32 * 32 * 3  # CIFAR-10 feature dimension

# ======================================================
# LOAD CIFAR-10 DATASET
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
y_train = y_train.astype("int32").flatten()
y_test  = y_test.astype("int32").flatten()

x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0

all_images = np.concatenate([x_train, x_test], axis=0)
all_labels = np.concatenate([y_train, y_test], axis=0)

print("ðŸ“¦ CIFAR-10 Loaded â†’ Total Samples:", len(all_images))

# ======================================================
# CREATE CLIENTS (IID + NonIID)
# ======================================================
def create_clients_iid_noniid(images, labels, num_clients, iid_indices, non_iid_indices):
    clients = {}
    scenario = {}

    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        samples_per_client = np.random.randint(800, 4000)
        client_samples = []

        if i in iid_indices:
            per_class = samples_per_client // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls], per_class, replace=True)
                client_samples.extend(chosen)
            scen = "IID"

        elif i in non_iid_indices:
            alpha = np.random.uniform(0.3, 3.0)
            class_weights = np.random.dirichlet(np.ones(10) * alpha)

            for cls in range(10):
                n_cls = int(class_weights[cls] * samples_per_client)
                if n_cls > 0:
                    chosen = np.random.choice(class_indices[cls], n_cls, replace=True)
                    client_samples.extend(chosen)
            scen = "NonIID"

        np.random.shuffle(client_samples)
        cid = i + 1
        clients[cid] = (images[client_samples], labels[client_samples])
        scenario[cid] = scen

    return clients, scenario

num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_indices     = list(range(0, num_iid))
non_iid_indices = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients_iid_noniid(
    all_images, all_labels,
    NUM_CLIENTS, iid_indices, non_iid_indices
)

print(f"âœ… Created {len(clients)} clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")

# ======================================================
# CIFAR-10 MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(32, (3,3), padding="same", activation="relu", input_shape=(32,32,3)),
        layers.Conv2D(32, (3,3), padding="same", activation="relu"),
        layers.MaxPooling2D((2,2)),

        layers.Conv2D(64, (3,3), padding="same", activation="relu"),
        layers.Conv2D(64, (3,3), padding="same", activation="relu"),
        layers.MaxPooling2D((2,2)),

        layers.Flatten(),
        layers.Dense(128, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model

# ======================================================
# LOCAL TRAINING + LOCAL QoS
# ======================================================
client_records = []
base_accuracy = {}
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c,
              epochs=LOCAL_EPOCHS,
              batch_size=BATCH_SIZE,
              verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000.0

    _, acc = model.evaluate(x_test, y_test, verbose=0)
    acc_pct = float(acc * 100.0)
    base_accuracy[cid] = acc_pct  # kept (not used when inactive anymore)

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    bw_time = time.time() - bw_t0
    BW_MBps = float(size_MB / bw_time)

    # ======================================================
    # SAVE CLIENT WEIGHTS IN GOOGLE DRIVE
    # ======================================================
    weight_path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(weight_path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    label_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": int(len(y_c)),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": weight_path,
    }

    for l in range(10):
        row[f"Label{l}"] = int(label_map.get(l, 0))

    client_records.append(row)

    local_results_for_global[cid] = {
        "samples": len(y_c),
        "data": (x_c, y_c),
    }

print(f"âœ… Local training finished for {len(client_records)} clients")

# ======================================================
# FEDERATED ROUNDS
# ======================================================
def fedavg_weights(active_clients, local_results, global_model_weights):
    total_samples = sum(local_results[c]["samples"] for c in active_clients)
    if total_samples == 0:
        return global_model_weights

    agg = [np.zeros_like(w) for w in global_model_weights]

    for c in active_clients:
        x_c, y_c = local_results[c]["data"]
        model = build_model()
        model.set_weights(global_model_weights)
        model.fit(x_c, y_c, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w_c = model.get_weights()
        weight_factor = local_results[c]["samples"] / total_samples
        for i, w in enumerate(w_c):
            agg[i] += w * weight_factor

    return agg

quality_history = {cid: [] for cid in clients.keys()}
availability_history = {cid: [] for cid in clients.keys()}

global_model = build_model()
global_weights = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active_clients = []

    for cid in clients.keys():
        is_active = (np.random.rand() < AVAILABILITY_PROB)
        availability_history[cid].append(1 if is_active else 0)
        if is_active:
            active_clients.append(cid)

    if active_clients:
        global_weights = fedavg_weights(
            active_clients, local_results_for_global, global_weights
        )
        global_model.set_weights(global_weights)

    # ======================================================
    # UPDATED: Quality_Factor uses 0 when client is inactive
    # ======================================================
    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1] == 1:
            _, acc_c = global_model.evaluate(x_c, y_c, verbose=0)
            acc_value = float(acc_c * 100.0)
        else:
            acc_value = 0.0  # âœ… inactive => no task performed => quality = 0

        quality_history[cid].append(acc_value)

    print(f"Round {rnd}: active clients = {len(active_clients)}")

# ======================================================
# RELIABILITY SCORE
# ======================================================
reliability_score = {}
for cid in clients.keys():
    activations = sum(availability_history[cid])
    reliability_score[cid] = activations / GLOBAL_ROUNDS

print("âœ… Quality_Factor (per-round accuracies w/ zeros for inactive) and Reliability_Score computed.")

# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)

df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(
    lambda cid: json.dumps(quality_history[int(cid)])
)

df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(
    lambda cid: float(np.mean(quality_history[int(cid)]))
)

df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(
    lambda cid: reliability_score[int(cid)]
)

print(df_clients.head())

df_clients.describe()

df_clients.to_csv("CIFAR10_Client_Profiles_For_Composability_30_30.csv")

"""**HAR Dataset**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG  (KEEP YOUR CURRENT FEDERATED LOGIC)
# ======================================================
NUM_CLIENTS        = 30
IID_FRACTION       = 0.4
LOCAL_EPOCHS       = 10
GLOBAL_ROUNDS      = 10
BATCH_SIZE         = 32
AVAILABILITY_PROB  = 0.6

# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/MLaaS_Weights_20_HAR"
os.makedirs(WEIGHTS_DIR, exist_ok=True)

# ======================================================
# LOAD YOUR HAR CSV DATASET
# - APPLY ORIGINAL PREPROCESSING LOGIC:
#   1) LabelEncode activity
#   2) LabelEncode subject (if exists)
#   3) StandardScale features
# ======================================================
file_path = "/content/drive/My Drive/Early Drift Detection/pamap2_final.csv"
df = pd.read_csv(file_path)
df = df.sample(frac=1).reset_index(drop=True)

# Encode activity
le_activity = LabelEncoder()
df["activity"] = le_activity.fit_transform(df["activity"])

# Encode subject if present (as in your original code)
if "subject" in df.columns:
    le_subject = LabelEncoder()
    df["subject"] = le_subject.fit_transform(df["subject"])

# Separate X/y like original:
# features = drop(['activity','subject']) ; target = activity
drop_cols = ["activity"]
if "subject" in df.columns:
    drop_cols.append("subject")

X = df.drop(drop_cols, axis=1).values.astype("float32")
y = df["activity"].values.astype("int32")

FEATURE_COUNT = X.shape[1]
NUM_CLASSES   = len(np.unique(y))

# GLOBAL TRAIN/TEST (keep your test_size=0.4 as in your current code)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=42, shuffle=True
)

# Standardize features (original code logic)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train).astype("float32")
X_test  = scaler.transform(X_test).astype("float32")

print(f"ðŸ“¦ HAR Loaded â†’ Train: {len(X_train)}, Test: {len(X_test)}, Features: {FEATURE_COUNT}, Classes: {NUM_CLASSES}")

# ======================================================
# CREATE IID + NON-IID CLIENTS (KEEP YOUR LOGIC)
# ======================================================
def create_clients(X, y, num_clients, iid_idx, non_iid_idx):
    clients = {}
    scenario = {}

    cls_idx = {cls: np.where(y == cls)[0] for cls in np.unique(y)}

    for i in range(num_clients):
        samples_per_client = np.random.randint(300, 2000)
        chosen = []

        if i in iid_idx:
            per = samples_per_client // NUM_CLASSES
            for cls in cls_idx:
                sel = np.random.choice(cls_idx[cls], per, replace=True)
                chosen.extend(sel)
            scen = "IID"
        else:
            alpha = np.random.uniform(0.3, 3.0)
            dist = np.random.dirichlet(np.ones(NUM_CLASSES) * alpha)
            for idx, cls in enumerate(cls_idx):
                n = int(dist[idx] * samples_per_client)
                if n > 0:
                    sel = np.random.choice(cls_idx[cls], n, replace=True)
                    chosen.extend(sel)
            scen = "NonIID"

        np.random.shuffle(chosen)
        cid = i + 1
        clients[cid] = (X[chosen], y[chosen])
        scenario[cid] = scen

    return clients, scenario

num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_idx     = list(range(0, num_iid))
non_iid_idx = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients(X_train, y_train, NUM_CLIENTS, iid_idx, non_iid_idx)

print(f"âœ… Created {len(clients)} clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")

# ======================================================
# BUILD HAR MLP MODEL
# - UPDATE TO MATCH YOUR ORIGINAL MLP DEPTH:
#   256 â†’ 128 â†’ 64 â†’ 32 â†’ softmax
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Input(shape=(FEATURE_COUNT,)),
        layers.Dense(256, activation="relu"),
        layers.Dropout(0.2),
        layers.Dense(128, activation="relu"),
        layers.Dropout(0.2),
        layers.Dense(64, activation="relu"),
        layers.Dropout(0.2),
        layers.Dense(32, activation="relu"),
        layers.Dense(NUM_CLASSES, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model

# ======================================================
# LOCAL TRAINING + LOCAL QoS
# ======================================================
client_records = []
base_accuracy = {}                 # kept for reference (not used when inactive anymore)
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c,
              epochs=LOCAL_EPOCHS,
              batch_size=BATCH_SIZE,
              verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000.0

    _, acc = model.evaluate(X_test, y_test, verbose=0)
    acc_pct = float(acc * 100.0)
    base_accuracy[cid] = acc_pct

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    bw_time = time.time() - bw_t0
    BW_MBps = float(size_MB / bw_time)

    # ======================================================
    # SAVE CLIENT WEIGHTS IN GOOGLE DRIVE (CONSISTENT NAMING)
    # ======================================================
    weight_path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(weight_path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    label_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": int(len(y_c)),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": weight_path,
    }

    for l in range(NUM_CLASSES):
        row[f"Label{l}"] = int(label_map.get(l, 0))

    client_records.append(row)

    local_results_for_global[cid] = {
        "samples": len(y_c),
        "data": (x_c, y_c),
    }

print(f"âœ… Local training finished for {len(client_records)} clients")

# ======================================================
# FEDERATED ROUNDS (FEDAVG)
# ======================================================
def fedavg_weights(active_clients, local_results, global_model_weights):
    total_samples = sum(local_results[c]["samples"] for c in active_clients)
    if total_samples == 0:
        return global_model_weights

    agg = [np.zeros_like(w) for w in global_model_weights]

    for c in active_clients:
        x_c, y_c = local_results[c]["data"]
        model = build_model()
        model.set_weights(global_model_weights)
        model.fit(x_c, y_c, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w_c = model.get_weights()
        weight_factor = local_results[c]["samples"] / total_samples
        for i, w in enumerate(w_c):
            agg[i] += w * weight_factor

    return agg

quality_history = {cid: [] for cid in clients.keys()}
availability_history = {cid: [] for cid in clients.keys()}

global_model = build_model()
global_weights = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active_clients = []

    for cid in clients.keys():
        is_active = (np.random.rand() < AVAILABILITY_PROB)
        availability_history[cid].append(1 if is_active else 0)
        if is_active:
            active_clients.append(cid)

    if active_clients:
        global_weights = fedavg_weights(
            active_clients, local_results_for_global, global_weights
        )
        global_model.set_weights(global_weights)

    # ======================================================
    # Quality_Factor: inactive => 0 (same as your MNIST final version)
    # ======================================================
    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1] == 1:
            _, acc_c = global_model.evaluate(x_c, y_c, verbose=0)
            acc_value = float(acc_c * 100.0)
        else:
            acc_value = 0.0

        quality_history[cid].append(acc_value)

    print(f"Round {rnd}: active clients = {len(active_clients)}")

# ======================================================
# RELIABILITY SCORE
# ======================================================
reliability_score = {}
for cid in clients.keys():
    activations = sum(availability_history[cid])
    reliability_score[cid] = activations / GLOBAL_ROUNDS

print("âœ… Quality_Factor (per-round accuracies w/ zeros for inactive) and Reliability_Score computed.")

# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)

df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(
    lambda cid: json.dumps(quality_history[int(cid)])
)
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(
    lambda cid: float(np.mean(quality_history[int(cid)]))
)
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(
    lambda cid: reliability_score[int(cid)]
)
print(df_clients.head())

df_clients.describe()

df_clients.to_csv("HAR_Client_Profiles_For_Composability.csv")